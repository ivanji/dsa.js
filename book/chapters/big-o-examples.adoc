= Big O examples
// = Eight Running Times You Should Know

There are many kinds of algorithms. Most of them fall into one of the eight of the time complexities that we are going to explore in this chapter.

.Most common time complexities
- Constant time: _O(1)_
- Logarithmic time: _O(log n)_
- Linear time: _O(n)_
- Linearithmic time: _O(n log n)_
- Quadratic time: _O(n^2^)_
- Cubic time: _O(n^3^)_
- Exponential time: _O(2^n^)_
- Factorial time: _O(n!)_

We a going to provide examples for each one of them.

Before we dive in, here‚Äôs a plot with all of them.

.CPU operations vs. Algorithm runtime as the input size grows
image:image5.png[CPU time needed vs. Algorithm runtime as the input size increases]

The above chart shows how the running time of an algorithm is related to the amount of work the CPU has to perform. As you can see O(1) and O(log n) are very scalable. However, O(n^2^) and worst can make your computer run for years [big]#üòµ# on large datasets. We are going to give some examples so you can identify each one.

== Constant

Represented as *O(1)*, it means that regardless of the input size the number of operations executed is always the same. Let‚Äôs see an example.

[#constant-example]
=== Finding if an array is empty

Let's implement a function that finds out if an array is empty or not.

//.is-empty.js
//image:image6.png[image,width=528,height=401]

[source, javascript]
----
include::{codedir}/runtimes/01-is-empty.js[tag=isEmpty]
----

Another more real life example is adding an element to the begining of a <<Linked List>>. You can check out the implementation <<linked-list-inserting-beginning, here>>.

As you can see, in both examples (array and linked list) if the input is a collection of 10 elements or 10M it would take the same amount of time to execute. You can't get any more performance than this!

== Logarithmic

Represented in Big O notation as *O(log n)*, when an algorithm has this running time it means that as the size of the input grows the number of operations grows very slowly. Logarithmic algorithms are very scalable. One example is the *binary search*.

[#logarithmic-example]
=== Searching on a sorted array

The binary search only works for sorted lists. It starts searching for an element on the middle of the array and then it moves to the right or left depending if the value you are looking for is bigger or smaller.

// image:image7.png[image,width=528,height=437]

[source, javascript]
----
include::{codedir}/runtimes/02-binary-search.js[tag=binarySearchRecursive]
----

This binary search implementation is a recursive algorithm, which means that the function `binarySearch` calls itself multiple times until the solution is found. The binary search split the array in half every time.

Finding the runtime of recursive algorithms is not very obvious sometimes. It requires some tools like recursion trees or the https://adrianmejia.com/blog/2018/04/24/analysis-of-recursive-algorithms/[Master Theorem]. The `binarySearch` divides the input in half each time. As a rule of thumb, when you have an algorithm that divides the data in half on each call you are most likely in front of a logarithmic runtime: _O(log n)_.

== Linear

Linear algorithms are one of the most common runtimes. It‚Äôs represented as *O(n)*. Usually, an algorithm has a linear running time when it iterates over all the elements in the input.

[#linear-example]
=== Finding duplicates in an array using a map

Let‚Äôs say that we want to find duplicate elements in an array. What‚Äôs the first implementation that comes to mind? Check out this implementation:

// image:image8.png[image,width=528,height=383]

[source, javascript]
----
include::{codedir}/runtimes/03-has-duplicates.js[tag=hasDuplicates]
----

.`hasDuplicates` has multiple scenarios:
* *Best-case scenario*: first two elements are duplicates. It only has to visit two elements.
* *Worst-case scenario*: no duplicated or duplicated are the last two. In either case, it has to visit every item on the array.
* *Average-case scenario*: duplicates are somewhere in the middle of the collection. Only, half of the array will be visited.

As we learned before, the big O cares about the worst-case scenario, where we would have to visit every element on the array. So, we have an *O(n)* runtime.

Space complexity is also *O(n)* since we are using an auxiliary data structure.  We have a map that in the worst case (no duplicates) it will hold every word.

== Linearithmic

An algorithm with a linearithmic runtime is represented as _O(n log n)_. This one is important because it is the best runtime for sorting! Let‚Äôs see the merge-sort.

[#linearithmic-example]
=== Sorting elements in an array

The merge sort, like its name indicates, has two functions merge and sort. Let‚Äôs start with the sort function:

// image:image9.png[image,width=528,height=383]

.Sort part of the mergeSort
[source, javascript]
----
include::{codedir}/runtimes/04-merge-sort.js[tag=sort]
----

Starting with the sort part, we divide the array into two halves and then merge them (line 16) recursively with the following function:

// image:image10.png[image,width=528,height=380]

.Merge part of the mergeSort
[source, javascript]
----
include::{codedir}/runtimes/04-merge-sort.js[tag=merge]
----

The merge function combines two sorted arrays in ascending order. Let‚Äôs say that we want to sort the array `[9, 2, 5, 1, 7, 6]`. In the following illustration, you can see what each function does.

.Mergesort visualization. Shows the split, sort and merge steps
image:image11.png[Mergesort visualization,width=500,height=600]

How do we obtain the running time of the merge sort algorithm? The mergesort divides the array in half each time in the split phase, _log n_, and the merge function join each splits, _n_. The total work we have *O(n log n)*. There more formal ways to reach to this runtime like using the https://adrianmejia.com/blog/2018/04/24/analysis-of-recursive-algorithms/[Master Method] and https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/lec20.html[recursion trees].

== Quadratic

Running times that are quadratic, O(n^2^), are the ones to watch out for. They usually don‚Äôt scale well when they have a large amount of data to process.

Usually, they have double-nested loops that where each one visits all or most elements in the input. One example of this is a na√Øve implementation to find duplicate words on an array.

[#quadratic-example]
=== Finding duplicates in an array (na√Øve approach)

If you remember we have solved this problem more efficiently on the <<Linear, Linear>> section. We solved this problem before using an _O(n)_, let‚Äôs solve it this time with an _O(n^2^)_:

// image:image12.png[image,width=527,height=389]

.Na√Øve implementation of has duplicates function
[source, javascript]
----
include::{codedir}/runtimes/05-has-duplicates-naive.js[tag=hasDuplicates]
----

As you can see, we have two nested loops causing the running time to be quadratic. How much different is a linear vs. quadratic algorithm?

Let‚Äôs say you want to find a duplicated middle name in a phone directory book of a city of ~1 million people. If you use this quadratic solution you would have to wait for ~12 days to get an answer [big]#üê¢#; while if you use the <<Linear, linear solution>> you will get the answer in seconds! [big]#üöÄ#

== Cubic

Cubic *O(n^3^)* and higher polynomial functions usually involve many nested loops. As an example of a cubic algorithm is a multi-variable equation solver (using brute force):

[#cubic-example]
=== Solving a multi-variable equation

Let‚Äôs say we want to find the solution for this multi-variable equation:

_3x + 9y + 8z = 79_

A na√Øve approach to solve this will be the following program:

//image:image13.png[image,width=528,height=448]

.Na√Øve implementation of multi-variable equation solver
[source, javascript]
----
include::{codedir}/runtimes/06-multi-variable-equation-solver.js[tag=findXYZ]
----

WARNING: This just an example, there are better ways to solve multi-variable equations.

As you can see three nested loops usually translates to O(n^3^). If you have a four variable equation and four nested loops it would be O(n^4^) and so on when we have a runtime in the form of _O(n^c^)_, where _c > 1_, we can refer as a *polynomial runtime*.

== Exponential

Exponential runtimes, O(2^n^), means that every time the input grows by one the number of operations doubles. Exponential programs are only usable for a tiny number of elements (<100) otherwise it might not finish on your lifetime.  [big]#üíÄ#

Let‚Äôs do an example.

[#exponential-example]
=== Finding subsets of a set

Finding all distinct subsets of a given set can be implemented as follows:

// image:image14.png[image,width=528,height=401]

.Subsets in a Set
[source, javascript]
----
include::{codedir}/runtimes/07-sub-sets.js[tag=snippet]
----
<1> Base case is empty element.
<2> For each element from the input append it to the results array.
<3> The new results array will be what it was before + the duplicated with the appended element.

//.The way this algorithm generates all subsets is:
//1.  The base case is an empty element (line 13). E.g. ['']
//2.  For each element from the input append it to the results array (line 16)
//3.  The new results array will be what it was before + the duplicated with the appended element (line 17)

Every time the input grows by one the resulting array doubles. That‚Äôs why it has an *O(2^n^)*.

== Factorial

Factorial runtime, O(n!), is not scalable at all. Even with input sizes of ~10 elements, it will take a couple of seconds to compute. It‚Äôs that slow! [big]*üçØüêù*

.Factorial
****
A factorial is the multiplication of all the numbers less than itself down to 1.

.For instance:
- 3! = 3 x 2 x 1 = 6
- 5! = 5 x 4 x 3 x 2 x 1 = 120
- 10! = 3,628,800
- 11! = 39,916,800
****

[#factorial-example]
=== Getting all permutations of a word

One classic example of an _O(n!)_ algorithm is finding all the different words that can be formed with a given set of letters.

.Word's permutations
// image:image15.png[image,width=528,height=377]
[source, javascript]
----
include::{codedir}/runtimes/08-permutations.js[tag=snippet]
----

As you can see in the `getPermutations` function, the resulting array is the factorial of the word length.

Factorial start very slow and then it quickly becomes uncontrollable. A word size of just 11 characters would take a couple of hours in most computers!
[big]*ü§Ø*

== Summary

We went through 8 of the most common time complexities and provided examples for each of them. Hopefully, this will give you a toolbox to analyze algorithms.

.Most common algorithmic running times and their examples
[cols="2,2,5",options="header"]
|===
|Big O Notation
|Name
|Example(s)

|O(1)
|<<Constant>>
|<<constant-example>>

|O(log n)
|<<Logarithmic>>
|<<logarithmic-example>>

|O(n)
|<<Linear>>
|<<linear-example>>

|O(n log n)
|<<Linearithmic>>
|<<linearithmic-example>>

|O(n^2^)
|<<Quadratic>>
|<<quadratic-example>>

|O(n^3^)
|<<Cubic>>
|<<cubic-example>>

|O(2^n^)
|<<Exponential>>
|<<exponential-example>>

|O(n!)
|<<Factorial>>
|<<factorial-example>>
|===
